{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMCcqE9CIAO+lOgFbptmXaA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lpoggetto/fiap_liveness/blob/main/trabalho_computer_vision_recog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Atenção!\n",
        "\n",
        "Recomendado utilizar instância com placa de vídeo!\n",
        "\n",
        "Ambiente de execução -> Alterar o tipo de ambiente de execução -> T4 GPU"
      ],
      "metadata": {
        "id": "P0lE9AaaiW_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Limpando a pasta do colab para iniciar um novo projeto"
      ],
      "metadata": {
        "id": "ARMAnDMFcR8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# limpando a pasta\n",
        "!rm -rf /content/*"
      ],
      "metadata": {
        "id": "XKMUclRKudSG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalando os pacotes utilizados no programa"
      ],
      "metadata": {
        "id": "TRhlnxoucVk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instalando pacotes utilizados\n",
        "!pip uninstall -y dlib\n",
        "\n",
        "\n",
        "!pip install git+https://github.com/hukkelas/DSFD-Pytorch-Inference.git\n",
        "!pip install face_detection\n",
        "!pip install mediapipe\n",
        "# !pip install dlib\n",
        "\n",
        "!apt-get update\n",
        "!apt-get install -y cmake libopenblas-dev liblapack-dev libx11-dev libgtk-3-dev\n",
        "!pip install dlib --no-binary :all:\n",
        "\n",
        "print('pacotes instalados')"
      ],
      "metadata": {
        "id": "LkvgAJsEWUCV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44c9f6ff-20f8-4f48-95e1-5bfe793140a2",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: dlib 19.24.6\n",
            "Uninstalling dlib-19.24.6:\n",
            "  Successfully uninstalled dlib-19.24.6\n",
            "Collecting git+https://github.com/hukkelas/DSFD-Pytorch-Inference.git\n",
            "  Cloning https://github.com/hukkelas/DSFD-Pytorch-Inference.git to /tmp/pip-req-build-ydwjsbtp\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/hukkelas/DSFD-Pytorch-Inference.git /tmp/pip-req-build-ydwjsbtp\n",
            "  Resolved https://github.com/hukkelas/DSFD-Pytorch-Inference.git to commit dde9c7dd9cdc9254c2ca345222c86a8ecfa17f5b\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_detection==0.2.1) (1.26.4)\n",
            "Requirement already satisfied: face_detection in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_detection) (1.26.4)\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.10/dist-packages (0.10.20)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.8.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.25.5)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "liblapack-dev is already the newest version (3.10.0-2ubuntu1).\n",
            "libopenblas-dev is already the newest version (0.3.20+ds-1).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "libgtk-3-dev is already the newest version (3.24.33-1ubuntu2.2).\n",
            "libx11-dev is already the newest version (2:1.7.5-1ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 50 not upgraded.\n",
            "Collecting dlib\n",
            "  Using cached dlib-19.24.6-cp310-cp310-linux_x86_64.whl\n",
            "Installing collected packages: dlib\n",
            "Successfully installed dlib-19.24.6\n",
            "pacotes instalados\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importando os pacotes utilizados"
      ],
      "metadata": {
        "id": "dHymk3FauK4t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eDA1up8gsJma"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import bz2\n",
        "import os\n",
        "import face_detection\n",
        "import mediapipe as mp\n",
        "from scipy.spatial import distance as dist\n",
        "from PIL import Image\n",
        "import warnings\n",
        "from google.colab import output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#Exibição na mesma tela do Jupyter\n",
        "%matplotlib inline\n",
        "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
        "\n",
        "# pacotes utilizados para tirar foto/videos\n",
        "from IPython.display import display, Javascript, Image, HTML\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import threading\n",
        "\n",
        "import dlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"dlib version:\", dlib.__version__)\n",
        "print(\"CUDA available:\", dlib.DLIB_USE_CUDA)"
      ],
      "metadata": {
        "id": "Zkb0AI-oHPfe",
        "outputId": "34b69be8-ed2c-485e-adc2-ca6a73470d1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dlib version: 19.24.6\n",
            "CUDA available: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criacao de pasta pasta para armazenamento dos videos"
      ],
      "metadata": {
        "id": "lPviDDTvSh8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# criando diretorios para armazenar as informacoes necessarias\n",
        "os.makedirs('videos', exist_ok=True) # validacao de video\n",
        "os.makedirs('foto', exist_ok=True) # validacao de video"
      ],
      "metadata": {
        "id": "gUpHEK6QGCZO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importando os modelos utilizados no reconhecimento facial"
      ],
      "metadata": {
        "id": "S4g0MfQAFh7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL e nome dos arquivos\n",
        "landmark_url = \"http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\"\n",
        "landmark_file_name = \"shape_predictor_68_face_landmarks.dat.bz2\"\n",
        "landmark_dat_file = \"shape_predictor_68_face_landmarks.dat\"\n",
        "\n",
        "recognition_url = \"http://dlib.net/files/dlib_face_recognition_resnet_model_v1.dat.bz2\"\n",
        "recognition_file_name = \"dlib_face_recognition_resnet_model_v1.dat.bz2\"\n",
        "recognition_dat_file = \"dlib_face_recognition_resnet_model_v1.dat\"\n",
        "\n",
        "# Criando diretório para salvar os modelos\n",
        "os.makedirs('dlib_models', exist_ok=True)\n",
        "\n",
        "# Apontando para o diretório\n",
        "os.chdir('dlib_models')\n",
        "\n",
        "# Função para baixar e extrair arquivos\n",
        "def download_and_extract(url, compressed_file_name, extracted_file_name):\n",
        "    # Baixando o arquivo se já não estiver baixado\n",
        "    if not os.path.exists(compressed_file_name):\n",
        "        os.system(f\"wget {url}\")\n",
        "\n",
        "    # Extraindo o arquivo .dat se já não estiver extraído\n",
        "    if not os.path.exists(extracted_file_name):\n",
        "        with bz2.BZ2File(compressed_file_name, \"rb\") as f_in, open(extracted_file_name, \"wb\") as f_out:\n",
        "            f_out.write(f_in.read())\n",
        "\n",
        "        # Apagando o arquivo compactado\n",
        "        os.remove(compressed_file_name)\n",
        "\n",
        "# Baixando e extraindo o arquivo de landmarks\n",
        "download_and_extract(landmark_url, landmark_file_name, landmark_dat_file)\n",
        "\n",
        "# Baixando e extraindo o arquivo de reconhecimento facial\n",
        "download_and_extract(recognition_url, recognition_file_name, recognition_dat_file)\n",
        "\n",
        "# Voltando para o diretório original\n",
        "os.chdir('..')\n",
        "\n",
        "print('Modelos baixados e extraídos com sucesso!')"
      ],
      "metadata": {
        "id": "ptCQRHaKFEui",
        "outputId": "20856a92-6f49-4ee9-9966-17a80a6ead8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelos baixados e extraídos com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Capturando foto \"original\" para validar faces"
      ],
      "metadata": {
        "id": "yXKAPDRQsmuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def capturar_foto_colab():\n",
        "    \"\"\"Captura uma imagem no Google Colab usando a webcam.\"\"\"\n",
        "    display(Javascript('''\n",
        "        async function captureImage() {\n",
        "            const div = document.createElement('div');\n",
        "            const captureButton = document.createElement('button');\n",
        "            captureButton.textContent = 'Capture';\n",
        "            div.appendChild(captureButton);\n",
        "            const video = document.createElement('video');\n",
        "            video.style.display = 'block';\n",
        "            const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "            document.body.appendChild(div);\n",
        "            div.appendChild(video);\n",
        "            video.srcObject = stream;\n",
        "            await video.play();\n",
        "            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "            await new Promise((resolve) => captureButton.onclick = resolve);\n",
        "            const canvas = document.createElement('canvas');\n",
        "            canvas.width = video.videoWidth;\n",
        "            canvas.height = video.videoHeight;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            stream.getVideoTracks()[0].stop();\n",
        "            div.remove();\n",
        "            return canvas.toDataURL('image/jpeg').split(',')[1];\n",
        "        }\n",
        "    '''))\n",
        "    image_data = output.eval_js('captureImage()')\n",
        "    with open('/content/foto/foto_base.jpg', 'wb') as f:\n",
        "        f.write(b64decode(image_data))\n",
        "    print(\"Imagem capturada e salva como 'foto_base.jpg'.\")\n",
        "\n",
        "capturar_foto_colab()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rCJYbie6smhH",
        "outputId": "4dba5978-777b-441b-eac4-d37c9f743265",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        async function captureImage() {\n",
              "            const div = document.createElement('div');\n",
              "            const captureButton = document.createElement('button');\n",
              "            captureButton.textContent = 'Capture';\n",
              "            div.appendChild(captureButton);\n",
              "            const video = document.createElement('video');\n",
              "            video.style.display = 'block';\n",
              "            const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "            document.body.appendChild(div);\n",
              "            div.appendChild(video);\n",
              "            video.srcObject = stream;\n",
              "            await video.play();\n",
              "            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "            await new Promise((resolve) => captureButton.onclick = resolve);\n",
              "            const canvas = document.createElement('canvas');\n",
              "            canvas.width = video.videoWidth;\n",
              "            canvas.height = video.videoHeight;\n",
              "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "            stream.getVideoTracks()[0].stop();\n",
              "            div.remove();\n",
              "            return canvas.toDataURL('image/jpeg').split(',')[1];\n",
              "        }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagem capturada e salva como 'foto_base.jpg'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Capturando video para aplicar liveness"
      ],
      "metadata": {
        "id": "swO_DS_3T6GM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JavaScript para capturar video\n",
        "html_code = \"\"\"\n",
        "<video id=\"video\" width=\"640\" height=\"480\" autoplay></video>\n",
        "<button id=\"startButton\">Start Recording</button>\n",
        "<button id=\"stopButton\" disabled>Stop Recording</button>\n",
        "<video id=\"playback\" width=\"640\" height=\"480\" controls></video>\n",
        "<a id=\"downloadLink\" download=\"recorded-video.webm\"></a>\n",
        "\n",
        "<script>\n",
        "    const video = document.querySelector('#video');\n",
        "    const playback = document.querySelector('#playback');\n",
        "    const downloadLink = document.querySelector('#downloadLink');\n",
        "    const startButton = document.querySelector('#startButton');\n",
        "    const stopButton = document.querySelector('#stopButton');\n",
        "\n",
        "    navigator.mediaDevices.getUserMedia({ video: true }).then((stream) => {\n",
        "        video.srcObject = stream;\n",
        "        let mediaRecorder;\n",
        "        const chunks = [];\n",
        "\n",
        "        startButton.onclick = () => {\n",
        "            mediaRecorder = new MediaRecorder(stream);\n",
        "            mediaRecorder.start();\n",
        "            startButton.disabled = true;\n",
        "            stopButton.disabled = false;\n",
        "\n",
        "            mediaRecorder.ondataavailable = (event) => chunks.push(event.data);\n",
        "            mediaRecorder.onstop = () => {\n",
        "                const blob = new Blob(chunks, { type: 'video/webm' });\n",
        "                playback.src = URL.createObjectURL(blob);\n",
        "                downloadLink.href = playback.src;\n",
        "                downloadLink.textContent = 'Download Video';\n",
        "                startButton.disabled = false;\n",
        "                stopButton.disabled = true;\n",
        "            };\n",
        "        };\n",
        "\n",
        "        stopButton.onclick = () => mediaRecorder.stop();\n",
        "    });\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "# Display the HTML interface\n",
        "display(HTML(html_code))"
      ],
      "metadata": {
        "id": "UR6wrfItjxqb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Montando detector de faces\n",
        "\n",
        "* Utilizando o algoritmo DSFDDetector para detectar uma face\n",
        "* Caso seja identificado que o usuario piscou os olhos, a prova de vida sera considerada valida"
      ],
      "metadata": {
        "id": "ANvOYVy9WL0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculo_distancia_olhos(eye):\n",
        "    # Calcula a distância euclidiana entre os pontos dos olhos\n",
        "    A = dist.euclidean(eye[1], eye[5])\n",
        "    B = dist.euclidean(eye[2], eye[4])\n",
        "    C = dist.euclidean(eye[0], eye[3])\n",
        "    # Calcula a razão de aspecto do olho\n",
        "    ear = (A + B) / (2.0 * C)\n",
        "    return ear\n",
        "\n",
        "def detector_faces_video(video_path, output_path, threshhold_ear=0.3, ref_image_path='/content/foto/foto_base.jpg', treshhold_recognition = 0.6):\n",
        "    # Criando o detector de faces\n",
        "    detector = face_detection.build_detector(\n",
        "        \"DSFDDetector\",\n",
        "        confidence_threshold=0.5,\n",
        "        nms_iou_threshold=0.3\n",
        "    )\n",
        "    # utilizando algoritmo de 68 pontos faciais\n",
        "    preditor_marcos_faciais = dlib.shape_predictor(\n",
        "        '/content/dlib_models/shape_predictor_68_face_landmarks.dat'\n",
        "        )\n",
        "\n",
        "    # algoritmo de face recognition\n",
        "    reconhecedor = dlib.face_recognition_model_v1(\n",
        "        '/content/dlib_models/dlib_face_recognition_resnet_model_v1.dat'\n",
        "        )\n",
        "\n",
        "    # Carregando a imagem de referência\n",
        "    ref_image = dlib.load_rgb_image(ref_image_path)\n",
        "    ref_detections = detector.detect(ref_image)\n",
        "\n",
        "    if len(ref_detections) == 0:\n",
        "        raise ValueError('Nenhuma face detectada na imagem de referência.')\n",
        "\n",
        "    x1, y1, x2, y2, score = map(int, ref_detections[0])\n",
        "    ref_dlib_rect = dlib.rectangle(x1, y1, x2, y2)\n",
        "    ref_landmarks = preditor_marcos_faciais(ref_image, ref_dlib_rect)\n",
        "    ref_embedding = np.array(reconhecedor.compute_face_descriptor(ref_image, ref_landmarks))\n",
        "\n",
        "    # abrindo o arquivo de video\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # propriedades do video\n",
        "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'VP80')  # WebM codec\n",
        "    out = cv2.VideoWriter(output_path, fourcc, 15, (width, height))\n",
        "\n",
        "    # iniciando variaveis para a validacao de prova vida\n",
        "    blink_detected = False\n",
        "    fl_liveness = 0\n",
        "\n",
        "    # loop para avaliar o video\n",
        "    while True:\n",
        "        ret, frame = video.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # convertendo para RGB\n",
        "        rgb_frame = frame[:, :, ::-1]\n",
        "\n",
        "        # detectando  face\n",
        "        detections = detector.detect(rgb_frame)\n",
        "\n",
        "        # a partir do momento em que e identificado que o usuario piscou os olhos\n",
        "        # a variavel de liveness redefine para 1\n",
        "        if blink_detected == True:\n",
        "            fl_liveness = 1\n",
        "\n",
        "        # desenhando o retangulo na face\n",
        "        for det in detections:\n",
        "            x1, y1, x2, y2, score = det\n",
        "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
        "\n",
        "\n",
        "            # utilizando dlib para detectar a face\n",
        "            dlib_rect = dlib.rectangle(x1,y1,x2,y2)\n",
        "\n",
        "            # marcos faciais\n",
        "            landmarks = preditor_marcos_faciais(rgb_frame, dlib_rect)\n",
        "\n",
        "            # coordenadas dos olhos\n",
        "            left_eye = [(landmarks.part(i).x, landmarks.part(i).y) for i in range(36, 42)]\n",
        "            right_eye = [(landmarks.part(i).x, landmarks.part(i).y) for i in range(42, 48)]\n",
        "\n",
        "            # calculando a distancia para ambos os olhos\n",
        "            olho_esq = calculo_distancia_olhos(left_eye)\n",
        "            olho_dir = calculo_distancia_olhos(right_eye)\n",
        "            ear = (olho_esq + olho_dir) / 2\n",
        "\n",
        "            # validacao de olhos piscando\n",
        "            blink_detected = ear < threshhold_ear\n",
        "\n",
        "            # computando a face embedding\n",
        "            face_embedding = np.array(reconhecedor.compute_face_descriptor(rgb_frame, landmarks))\n",
        "\n",
        "            # Comparando com a face de referência\n",
        "            similarity = 1 - dist.cosine(ref_embedding, face_embedding)\n",
        "\n",
        "            # Reconhecimento facial\n",
        "            if similarity > treshhold_recognition:\n",
        "                recognition_text = \"Reconhecido\"\n",
        "                color = (0, 255, 0)  # Verde\n",
        "            else:\n",
        "                recognition_text = \"Nao Reconhecido\"\n",
        "                color = (0, 0, 255)  # Vermelho\n",
        "\n",
        "            # Caixa ao redor da face\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "            cv2.putText(frame, recognition_text, (x1, y1 - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "            # Checando se identifica o usuário piscando os olhos\n",
        "            if fl_liveness == 1:\n",
        "                # Caixa verde - Liveness OK\n",
        "                cv2.putText(frame, 'Liveness OK', (x1, y2 + 20),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "            else:\n",
        "                # Caixa vermelha - Liveness não OK\n",
        "                cv2.putText(frame, 'Liveness nao OK', (x1, y2 + 20),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "\n",
        "        # retorno do video\n",
        "        out.write(frame)\n",
        "\n",
        "    # Release resources\n",
        "    video.release()\n",
        "    out.release()\n",
        "    print(f\"Video salvo em: {output_path}\")\n",
        "\n",
        "# variaveis para uso da funcao\n",
        "input_video_path = '/content/videos/recorded-video.webm'\n",
        "output_video_path = '/content/videos/recorded-video_processado.webm'\n",
        "ref_image_path = '/content/foto/foto_base.jpg'\n",
        "\n",
        "# chamada da funcao criada anteriormente\n",
        "detector_faces_video(input_video_path, output_video_path, threshhold_ear=0.3, ref_image_path=ref_image_path, treshhold_recognition=0.6)"
      ],
      "metadata": {
        "id": "tCX9NZJRZS6k",
        "outputId": "694b141c-2eac-43f8-b62e-009b0febcf4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error while calling cudaGetDevice(&the_device_id) in file /tmp/pip-install-4ukagg0_/dlib_8e5c4a6b9fbc4884804eca21649b2ede/dlib/cuda/gpu_data.cpp:204. code: 35, reason: CUDA driver version is insufficient for CUDA runtime version",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-60aab993214d>\u001b[0m in \u001b[0;36m<cell line: 136>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;31m# chamada da funcao criada anteriormente\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m \u001b[0mdetector_faces_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_video_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_video_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshhold_ear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_image_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref_image_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreshhold_recognition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-60aab993214d>\u001b[0m in \u001b[0;36mdetector_faces_video\u001b[0;34m(video_path, output_path, threshhold_ear, ref_image_path, treshhold_recognition)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# algoritmo de face recognition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     reconhecedor = dlib.face_recognition_model_v1(\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;34m'/content/dlib_models/dlib_face_recognition_resnet_model_v1.dat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error while calling cudaGetDevice(&the_device_id) in file /tmp/pip-install-4ukagg0_/dlib_8e5c4a6b9fbc4884804eca21649b2ede/dlib/cuda/gpu_data.cpp:204. code: 35, reason: CUDA driver version is insufficient for CUDA runtime version"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CrC5McAQvsQi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}