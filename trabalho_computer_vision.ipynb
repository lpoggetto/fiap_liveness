{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOsGfoWf5jWPxPdJ8+ZOnp2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lpoggetto/fiap_liveness/blob/main/trabalho_computer_vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Limpando a pasta do colab para iniciar um novo projeto"
      ],
      "metadata": {
        "id": "ARMAnDMFcR8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# limpando a pasta\n",
        "!rm -rf /content/*"
      ],
      "metadata": {
        "id": "XKMUclRKudSG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalando os pacotes utilizados no programa"
      ],
      "metadata": {
        "id": "TRhlnxoucVk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # instalando pacotes utilizados\n",
        "\n",
        "!pip install git+https://github.com/hukkelas/DSFD-Pytorch-Inference.git\n",
        "!pip install face_detection\n",
        "!pip install mediapipe\n",
        "!pip install dlib\n",
        "\n",
        "print('pacotes instalados')"
      ],
      "metadata": {
        "id": "LkvgAJsEWUCV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "312c1287-32a5-4912-c50b-4d7f1670afdc",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/hukkelas/DSFD-Pytorch-Inference.git\n",
            "  Cloning https://github.com/hukkelas/DSFD-Pytorch-Inference.git to /tmp/pip-req-build-_6x9nd1p\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/hukkelas/DSFD-Pytorch-Inference.git /tmp/pip-req-build-_6x9nd1p\n",
            "  Resolved https://github.com/hukkelas/DSFD-Pytorch-Inference.git to commit dde9c7dd9cdc9254c2ca345222c86a8ecfa17f5b\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_detection==0.2.1) (1.26.4)\n",
            "Requirement already satisfied: face_detection in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_detection) (1.26.4)\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.10/dist-packages (0.10.20)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.8.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.25.5)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Requirement already satisfied: dlib in /usr/local/lib/python3.10/dist-packages (19.24.2)\n",
            "pacotes instalados\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importando os pacotes utilizados"
      ],
      "metadata": {
        "id": "dHymk3FauK4t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eDA1up8gsJma"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import bz2\n",
        "import os\n",
        "import face_detection\n",
        "import mediapipe as mp\n",
        "from scipy.spatial import distance as dist\n",
        "import dlib\n",
        "from PIL import Image\n",
        "import warnings\n",
        "from google.colab import output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#Exibição na mesma tela do Jupyter\n",
        "%matplotlib inline\n",
        "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
        "\n",
        "# pacotes utilizados para tirar foto/videos\n",
        "from IPython.display import display, Javascript, Image, HTML\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import threading\n",
        "\n",
        "dlib.DLIB_USE_CUDA = True"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criacao de pasta pasta para armazenamento dos videos"
      ],
      "metadata": {
        "id": "lPviDDTvSh8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# criando diretorios para armazenar as informacoes necessarias\n",
        "os.makedirs('videos', exist_ok=True) # validacao de video"
      ],
      "metadata": {
        "id": "gUpHEK6QGCZO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importando os modelos utilizados no reconhecimento facial"
      ],
      "metadata": {
        "id": "S4g0MfQAFh7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL e nome dos arquivos\n",
        "landmark_url = \"http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\"\n",
        "landmark_file_name = \"shape_predictor_68_face_landmarks.dat.bz2\"\n",
        "landmark_dat_file = \"shape_predictor_68_face_landmarks.dat\"\n",
        "\n",
        "recognition_url = \"http://dlib.net/files/dlib_face_recognition_resnet_model_v1.dat.bz2\"\n",
        "recognition_file_name = \"dlib_face_recognition_resnet_model_v1.dat.bz2\"\n",
        "recognition_dat_file = \"dlib_face_recognition_resnet_model_v1.dat\"\n",
        "\n",
        "# Criando diretório para salvar os modelos\n",
        "os.makedirs('dlib_models', exist_ok=True)\n",
        "\n",
        "# Apontando para o diretório\n",
        "os.chdir('dlib_models')\n",
        "\n",
        "# Função para baixar e extrair arquivos\n",
        "def download_and_extract(url, compressed_file_name, extracted_file_name):\n",
        "    # Baixando o arquivo se já não estiver baixado\n",
        "    if not os.path.exists(compressed_file_name):\n",
        "        os.system(f\"wget {url}\")\n",
        "\n",
        "    # Extraindo o arquivo .dat se já não estiver extraído\n",
        "    if not os.path.exists(extracted_file_name):\n",
        "        with bz2.BZ2File(compressed_file_name, \"rb\") as f_in, open(extracted_file_name, \"wb\") as f_out:\n",
        "            f_out.write(f_in.read())\n",
        "\n",
        "        # Apagando o arquivo compactado\n",
        "        os.remove(compressed_file_name)\n",
        "\n",
        "# Baixando e extraindo o arquivo de landmarks\n",
        "download_and_extract(landmark_url, landmark_file_name, landmark_dat_file)\n",
        "\n",
        "# Baixando e extraindo o arquivo de reconhecimento facial\n",
        "download_and_extract(recognition_url, recognition_file_name, recognition_dat_file)\n",
        "\n",
        "# Voltando para o diretório original\n",
        "os.chdir('..')\n",
        "\n",
        "print('Modelos baixados e extraídos com sucesso!')"
      ],
      "metadata": {
        "id": "ptCQRHaKFEui",
        "outputId": "d00af3d5-c549-49aa-b14b-719d8cb95493",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelos baixados e extraídos com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Capturando video para aplicar liveness"
      ],
      "metadata": {
        "id": "swO_DS_3T6GM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JavaScript para capturar video\n",
        "html_code = \"\"\"\n",
        "<video id=\"video\" width=\"640\" height=\"480\" autoplay></video>\n",
        "<button id=\"startButton\">Start Recording</button>\n",
        "<button id=\"stopButton\" disabled>Stop Recording</button>\n",
        "<video id=\"playback\" width=\"640\" height=\"480\" controls></video>\n",
        "<a id=\"downloadLink\" download=\"recorded-video.webm\"></a>\n",
        "\n",
        "<script>\n",
        "    const video = document.querySelector('#video');\n",
        "    const playback = document.querySelector('#playback');\n",
        "    const downloadLink = document.querySelector('#downloadLink');\n",
        "    const startButton = document.querySelector('#startButton');\n",
        "    const stopButton = document.querySelector('#stopButton');\n",
        "\n",
        "    navigator.mediaDevices.getUserMedia({ video: true }).then((stream) => {\n",
        "        video.srcObject = stream;\n",
        "        let mediaRecorder;\n",
        "        const chunks = [];\n",
        "\n",
        "        startButton.onclick = () => {\n",
        "            mediaRecorder = new MediaRecorder(stream);\n",
        "            mediaRecorder.start();\n",
        "            startButton.disabled = true;\n",
        "            stopButton.disabled = false;\n",
        "\n",
        "            mediaRecorder.ondataavailable = (event) => chunks.push(event.data);\n",
        "            mediaRecorder.onstop = () => {\n",
        "                const blob = new Blob(chunks, { type: 'video/webm' });\n",
        "                playback.src = URL.createObjectURL(blob);\n",
        "                downloadLink.href = playback.src;\n",
        "                downloadLink.textContent = 'Download Video';\n",
        "                startButton.disabled = false;\n",
        "                stopButton.disabled = true;\n",
        "            };\n",
        "        };\n",
        "\n",
        "        stopButton.onclick = () => mediaRecorder.stop();\n",
        "    });\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "# Display the HTML interface\n",
        "display(HTML(html_code))"
      ],
      "metadata": {
        "id": "UR6wrfItjxqb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Montando detector de faces\n",
        "\n",
        "* Utilizando o algoritmo DSFDDetector para detectar uma face\n",
        "* Caso seja identificado que o usuario piscou os olhos, a prova de vida sera considerada valida"
      ],
      "metadata": {
        "id": "ANvOYVy9WL0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculo_distancia_olhos(eye):\n",
        "    # Calcula a distância euclidiana entre os pontos dos olhos\n",
        "    A = dist.euclidean(eye[1], eye[5])\n",
        "    B = dist.euclidean(eye[2], eye[4])\n",
        "    C = dist.euclidean(eye[0], eye[3])\n",
        "    # Calcula a razão de aspecto do olho\n",
        "    ear = (A + B) / (2.0 * C)\n",
        "    return ear\n",
        "\n",
        "def detector_faces_video(video_path, output_path, threshhold_ear=0.3):\n",
        "    # Criando o detector de faces\n",
        "    detector = face_detection.build_detector(\n",
        "        \"DSFDDetector\",\n",
        "        confidence_threshold=0.5,\n",
        "        nms_iou_threshold=0.3\n",
        "    )\n",
        "    # utilizando algoritmo de 68 pontos faciais\n",
        "    preditor_marcos_faciais = dlib.shape_predictor(\n",
        "        '/content/dlib_models/shape_predictor_68_face_landmarks.dat'\n",
        "        )\n",
        "\n",
        "    # abrindo o arquivo de video\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # propriedades do video\n",
        "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'VP80')  # WebM codec\n",
        "    out = cv2.VideoWriter(output_path, fourcc, 15, (width, height))\n",
        "\n",
        "    # iniciando variaveis para a validacao de prova vida\n",
        "    blink_detected = False\n",
        "\n",
        "    fl_liveness = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = video.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # convertendo para RGB\n",
        "        rgb_frame = frame[:, :, ::-1]\n",
        "\n",
        "        # detectando  face\n",
        "        detections = detector.detect(rgb_frame)\n",
        "\n",
        "        # a partir do momento em que e identificado que o usuario piscou os olhos\n",
        "        # a variavel de liveness redefine para 1\n",
        "        if blink_detected == True:\n",
        "            fl_liveness = 1\n",
        "\n",
        "        # desenhando o retangulo na face\n",
        "        for det in detections:\n",
        "            x1, y1, x2, y2, score = det\n",
        "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
        "\n",
        "\n",
        "            # utilizando dlib para detectar a face\n",
        "            dlib_rect = dlib.rectangle(x1,y1,x2,y2)\n",
        "\n",
        "            # marcos faciais\n",
        "            landmarks = preditor_marcos_faciais(rgb_frame, dlib_rect)\n",
        "\n",
        "            # coordenadas dos olhos\n",
        "            left_eye = [(landmarks.part(i).x, landmarks.part(i).y) for i in range(36, 42)]\n",
        "            right_eye = [(landmarks.part(i).x, landmarks.part(i).y) for i in range(42, 48)]\n",
        "\n",
        "            # calculando a distancia para ambos os olhos\n",
        "            olho_esq = calculo_distancia_olhos(left_eye)\n",
        "            olho_dir = calculo_distancia_olhos(right_eye)\n",
        "            ear = (olho_esq + olho_dir) / 2\n",
        "\n",
        "            # validacao de olhos piscando\n",
        "            blink_detected = ear < threshhold_ear\n",
        "\n",
        "            # checando se identifica o usuario piscando os olhos\n",
        "            if fl_liveness == 1:\n",
        "                # Caixa verde - Liveness OK\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                cv2.putText(frame, 'Liveness OK', (x1, y1 - 10),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "            else:\n",
        "                # Caixa verde - Liveness Nao OK\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "                cv2.putText(frame, 'Liveness nao OK', (x1, y1 - 10),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "\n",
        "        # retorno do video\n",
        "        out.write(frame)\n",
        "\n",
        "    # Release resources\n",
        "    video.release()\n",
        "    out.release()\n",
        "    print(f\"Video salvo em: {output_path}\")\n",
        "\n",
        "# variaveis para uso da funcao\n",
        "input_video_path = '/content/videos/recorded-video.webm'\n",
        "output_video_path = '/content/videos/recorded-video_processado.webm'\n",
        "\n",
        "# chamada da funcao criada anteriormente\n",
        "detector_faces_video(input_video_path, output_video_path, threshhold_ear = 0.3)"
      ],
      "metadata": {
        "id": "tCX9NZJRZS6k",
        "outputId": "304aa7c2-a4db-4a57-a87c-849761401871",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video salvo em: /content/videos/recorded-video_processado.webm\n"
          ]
        }
      ]
    }
  ]
}