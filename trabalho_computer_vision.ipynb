{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNBNZkg1B7NElRCXPwsQHN/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lpoggetto/fiap_liveness/blob/main/trabalho_computer_vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir imagens\n",
        "!rm -rf sample_data"
      ],
      "metadata": {
        "id": "XKMUclRKudSG",
        "outputId": "732b9d1f-5f52-40e5-87be-5f91c18397a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘imagens’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # instalando pacotes utilizados\n",
        "\n",
        " !pip install git+https://github.com/hukkelas/DSFD-Pytorch-Inference.git\n",
        " !pip install face_detection\n",
        " !pip install mediapipe\n",
        " !pip install dlib"
      ],
      "metadata": {
        "id": "LkvgAJsEWUCV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6fd0b2f-6eff-4710-d2e9-6752897a65cd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/hukkelas/DSFD-Pytorch-Inference.git\n",
            "  Cloning https://github.com/hukkelas/DSFD-Pytorch-Inference.git to /tmp/pip-req-build-uxowmn5n\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/hukkelas/DSFD-Pytorch-Inference.git /tmp/pip-req-build-uxowmn5n\n",
            "  Resolved https://github.com/hukkelas/DSFD-Pytorch-Inference.git to commit dde9c7dd9cdc9254c2ca345222c86a8ecfa17f5b\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_detection==0.2.1) (1.26.4)\n",
            "Requirement already satisfied: face_detection in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_detection) (1.26.4)\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.10/dist-packages (0.10.18)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.8.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.25.5)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Requirement already satisfied: dlib in /usr/local/lib/python3.10/dist-packages (19.24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importando os pacotes utilizados"
      ],
      "metadata": {
        "id": "dHymk3FauK4t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "eDA1up8gsJma"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import bz2\n",
        "import os\n",
        "try:\n",
        "  import face_detection\n",
        "except:\n",
        "  !pip install face_detection\n",
        "  import face_detection\n",
        "\n",
        "try:\n",
        "  import mediapipe as mp\n",
        "except:\n",
        "  import mediapipe as mp\n",
        "from scipy.spatial import distance as dist\n",
        "import dlib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#Exibição na mesma tela do Jupyter\n",
        "%matplotlib inline\n",
        "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
        "\n",
        "# pacotes utilizados para tirar foto\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "from flask import Flask, request, send_from_directory\n",
        "import threading"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# URL of the pre-trained model\n",
        "url = \"http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\"\n",
        "file_name = \"shape_predictor_68_face_landmarks.dat.bz2\"\n",
        "\n",
        "# Download the file if it doesn't exist\n",
        "if not os.path.exists(file_name):\n",
        "  !wget {url}\n",
        "\n",
        "# Extract the .dat file if it doesn't exist\n",
        "dat_file = \"shape_predictor_68_face_landmarks.dat\"\n",
        "if not os.path.exists(dat_file):\n",
        "  with bz2.BZ2File(file_name, \"rb\") as f_in, open(dat_file, \"wb\") as f_out:\n",
        "    f_out.write(f_in.read())"
      ],
      "metadata": {
        "id": "ptCQRHaKFEui"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "a4AZp9ofuOMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# funcao para tirar foto\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename\n",
        "\n",
        "try:\n",
        "  filename = take_photo(\"imagens/foto.jpg\")\n",
        "  print('Saved to {}'.format(filename))\n",
        "\n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "metadata": {
        "id": "4xXd6xJwsVKE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "abbc0e52-95cd-43d3-dd23-513ec76dd81a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function takePhoto(quality) {\n",
              "      const div = document.createElement('div');\n",
              "      const capture = document.createElement('button');\n",
              "      capture.textContent = 'Capture';\n",
              "      div.appendChild(capture);\n",
              "\n",
              "      const video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "\n",
              "      document.body.appendChild(div);\n",
              "      div.appendChild(video);\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      // Resize the output to fit the video element.\n",
              "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "\n",
              "      // Wait for Capture to be clicked.\n",
              "      await new Promise((resolve) => capture.onclick = resolve);\n",
              "\n",
              "      const canvas = document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "      stream.getVideoTracks()[0].stop();\n",
              "      div.remove();\n",
              "      return canvas.toDataURL('image/jpeg', quality);\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to imagens/foto.jpg\n",
            "'module' object is not callable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "# JavaScript code to capture video\n",
        "html_code = \"\"\"\n",
        "<video id=\"video\" width=\"640\" height=\"480\" autoplay></video>\n",
        "<button id=\"startButton\">Start Recording</button>\n",
        "<button id=\"stopButton\" disabled>Stop Recording</button>\n",
        "<video id=\"playback\" width=\"640\" height=\"480\" controls></video>\n",
        "<a id=\"downloadLink\" download=\"recorded-video.webm\"></a>\n",
        "\n",
        "<script>\n",
        "    const video = document.querySelector('#video');\n",
        "    const playback = document.querySelector('#playback');\n",
        "    const downloadLink = document.querySelector('#downloadLink');\n",
        "    const startButton = document.querySelector('#startButton');\n",
        "    const stopButton = document.querySelector('#stopButton');\n",
        "\n",
        "    navigator.mediaDevices.getUserMedia({ video: true }).then((stream) => {\n",
        "        video.srcObject = stream;\n",
        "        let mediaRecorder;\n",
        "        const chunks = [];\n",
        "\n",
        "        startButton.onclick = () => {\n",
        "            mediaRecorder = new MediaRecorder(stream);\n",
        "            mediaRecorder.start();\n",
        "            startButton.disabled = true;\n",
        "            stopButton.disabled = false;\n",
        "\n",
        "            mediaRecorder.ondataavailable = (event) => chunks.push(event.data);\n",
        "            mediaRecorder.onstop = () => {\n",
        "                const blob = new Blob(chunks, { type: 'video/webm' });\n",
        "                playback.src = URL.createObjectURL(blob);\n",
        "                downloadLink.href = playback.src;\n",
        "                downloadLink.textContent = 'Download Video';\n",
        "                startButton.disabled = false;\n",
        "                stopButton.disabled = true;\n",
        "            };\n",
        "        };\n",
        "\n",
        "        stopButton.onclick = () => mediaRecorder.stop();\n",
        "    });\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "# Display the HTML interface\n",
        "display(HTML(html_code))"
      ],
      "metadata": {
        "id": "UR6wrfItjxqb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "# Ensure the output folder exists\n",
        "output_folder = \"/content/videos/\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# HTML + JavaScript for video recording and upload\n",
        "html_code = \"\"\"\n",
        "<video id=\"video\" width=\"640\" height=\"480\" autoplay></video>\n",
        "<button id=\"startButton\">Start Recording</button>\n",
        "<button id=\"stopButton\" disabled>Stop Recording</button>\n",
        "<video id=\"playback\" width=\"640\" height=\"480\" controls></video>\n",
        "<a id=\"downloadLink\" style=\"display: none;\"></a>\n",
        "\n",
        "<script>\n",
        "    const video = document.querySelector('#video');\n",
        "    const playback = document.querySelector('#playback');\n",
        "    const downloadLink = document.querySelector('#downloadLink');\n",
        "    const startButton = document.querySelector('#startButton');\n",
        "    const stopButton = document.querySelector('#stopButton');\n",
        "\n",
        "    navigator.mediaDevices.getUserMedia({ video: true }).then((stream) => {\n",
        "        video.srcObject = stream;\n",
        "        let mediaRecorder;\n",
        "        const chunks = [];\n",
        "\n",
        "        startButton.onclick = () => {\n",
        "            mediaRecorder = new MediaRecorder(stream);\n",
        "            mediaRecorder.start();\n",
        "            startButton.disabled = true;\n",
        "            stopButton.disabled = false;\n",
        "\n",
        "            mediaRecorder.ondataavailable = (event) => chunks.push(event.data);\n",
        "            mediaRecorder.onstop = () => {\n",
        "                const blob = new Blob(chunks, { type: 'video/webm' });\n",
        "                const formData = new FormData();\n",
        "                formData.append(\"file\", blob, \"recorded-video.webm\");\n",
        "\n",
        "                // Send the video blob to Python backend\n",
        "                fetch(\"/upload\", {\n",
        "                    method: \"POST\",\n",
        "                    body: formData\n",
        "                }).then(response => response.text())\n",
        "                  .then(result => alert(\"Video uploaded successfully!\"));\n",
        "\n",
        "                playback.src = URL.createObjectURL(blob);\n",
        "                startButton.disabled = false;\n",
        "                stopButton.disabled = true;\n",
        "            };\n",
        "        };\n",
        "\n",
        "        stopButton.onclick = () => mediaRecorder.stop();\n",
        "    });\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "# Backend endpoint to handle file uploads\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/upload', methods=['POST'])\n",
        "def upload():\n",
        "    file = request.files['file']\n",
        "    file_path = os.path.join(output_folder, file.filename)\n",
        "    file.save(file_path)\n",
        "    return \"File uploaded successfully!\"\n",
        "\n",
        "# Start the Flask app in a separate thread\n",
        "def start_server():\n",
        "    app.run(port=8000, debug=False)\n",
        "\n",
        "thread = threading.Thread(target=start_server)\n",
        "thread.start()\n",
        "\n",
        "# Display the HTML interface\n",
        "display(HTML(html_code))"
      ],
      "metadata": {
        "id": "3YId-NG0I-yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Montando detector de faces\n",
        "\n",
        "Utilizando o algoritmo DSFDDetector para detectar uma face"
      ],
      "metadata": {
        "id": "ANvOYVy9WL0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detector_faces(path_foto):\n",
        "  # Criando o detector de faces\n",
        "  detector = face_detection.build_detector(\n",
        "        \"DSFDDetector\",\n",
        "        confidence_threshold=.5,\n",
        "        nms_iou_threshold=.3\n",
        "        )\n",
        "\n",
        "  # Convertendo BGR -> RGB\n",
        "  im = cv2.imread(path_foto)[:, :, ::-1]\n",
        "\n",
        "  # array com as faces detectadas\n",
        "  detections = detector.detect(im)\n",
        "\n",
        "  # iterando para desenhar retangulo na face\n",
        "  for det in detections:\n",
        "      x1, y1, x2, y2, score = det  # obtendo resultados\n",
        "      x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])  # convertendo para int\n",
        "\n",
        "      # array na formatacao\n",
        "      im = im.astype(np.uint8)\n",
        "\n",
        "      # desenhando o retangulo na face detectada\n",
        "      cv2.rectangle(im, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "\n",
        "# Plot the image with bounding boxes and lines\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  plt.imshow(im)\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "CwHZZXMgUV3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculo_distancia_olhos(eye):\n",
        "    # Calcula a distância euclidiana entre os pontos dos olhos\n",
        "    A = dist.euclidean(eye[1], eye[5])\n",
        "    B = dist.euclidean(eye[2], eye[4])\n",
        "    C = dist.euclidean(eye[0], eye[3])\n",
        "    # Calcula a razão de aspecto do olho\n",
        "    ear = (A + B) / (2.0 * C)\n",
        "    return ear\n",
        "\n",
        "def detector_faces_video(video_path, output_path, threshhold_ear=0.3):\n",
        "    # Criando o detector de faces\n",
        "    detector = face_detection.build_detector(\n",
        "        \"DSFDDetector\",\n",
        "        confidence_threshold=0.5,\n",
        "        nms_iou_threshold=0.3\n",
        "    )\n",
        "    # utilizando algoritmo de 68 pontos faciais\n",
        "    preditor_marcos_faciais = dlib.shape_predictor(\n",
        "        '/content/shape_predictor_68_face_landmarks.dat'\n",
        "        )\n",
        "\n",
        "    # abrindo o arquivo de video\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # propriedades do video\n",
        "    fps = int(video.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'VP80')  # WebM codec\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    while True:\n",
        "        ret, frame = video.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # convertendo para RGB\n",
        "        rgb_frame = frame[:, :, ::-1]\n",
        "\n",
        "        # detectando  face\n",
        "        detections = detector.detect(rgb_frame)\n",
        "\n",
        "        # desenhando o retangulo na face\n",
        "        for det in detections:\n",
        "            x1, y1, x2, y2, score = det\n",
        "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
        "\n",
        "            # utilizando dlib para detectar a face\n",
        "            dlib_rect = dlib.rectangle(x1,y1,x2,y2)\n",
        "\n",
        "            # marcos faciais\n",
        "            landmarks = preditor_marcos_faciais(rgb_frame, dlib_rect)\n",
        "\n",
        "            # coordenadas dos olhos\n",
        "            left_eye = [(landmarks.part(i).x, landmarks.part(i).y) for i in range(36, 42)]\n",
        "            right_eye = [(landmarks.part(i).x, landmarks.part(i).y) for i in range(42, 48)]\n",
        "\n",
        "            # calculando a distancia para ambos os olhos\n",
        "            olho_esq = calculo_distancia_olhos(left_eye)\n",
        "            olho_dir = calculo_distancia_olhos(right_eye)\n",
        "            ear = (olho_esq + olho_dir) / 2\n",
        "\n",
        "            # validacao de olhos piscando\n",
        "            blink_detected = ear < threshhold_ear\n",
        "            if ear < threshhold_ear:\n",
        "              cv2.putText(frame, 'Cliente Piscou', (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "              cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            else:\n",
        "              cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "\n",
        "        # retorno do video\n",
        "        out.write(frame)\n",
        "\n",
        "    # Release resources\n",
        "    video.release()\n",
        "    out.release()\n",
        "    print(f\"Processed video saved to: {output_path}\")\n",
        "\n",
        "# Example usage\n",
        "input_video_path = '/content/imagens/recorded-video.webm'\n",
        "output_video_path = '/content/imagens/recorded-video_processado.webm'\n",
        "\n",
        "# chamada da funcao criada anteriormente\n",
        "detector_faces_video(input_video_path, output_video_path)"
      ],
      "metadata": {
        "id": "tCX9NZJRZS6k",
        "outputId": "6142e23a-3805-4938-e007-89a7538b178a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed video saved to: /content/imagens/recorded-video_processado.webm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QHL15wFGHdZy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}