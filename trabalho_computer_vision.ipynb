{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMBPXWktIK0dhGrm1drA/KU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lpoggetto/fiap_liveness/blob/main/trabalho_computer_vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# limpando a pasta\n",
        "!rm -rf /content/*"
      ],
      "metadata": {
        "id": "XKMUclRKudSG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # instalando pacotes utilizados\n",
        "\n",
        "!pip install git+https://github.com/hukkelas/DSFD-Pytorch-Inference.git\n",
        "!pip install face_detection\n",
        "!pip install mediapipe\n",
        "!pip install dlib\n",
        "\n",
        "print('pacotes instalados')"
      ],
      "metadata": {
        "id": "LkvgAJsEWUCV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbaaa203-fc18-4724-b3bf-b17dce8f9889"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/hukkelas/DSFD-Pytorch-Inference.git\n",
            "  Cloning https://github.com/hukkelas/DSFD-Pytorch-Inference.git to /tmp/pip-req-build-fa1mzstq\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/hukkelas/DSFD-Pytorch-Inference.git /tmp/pip-req-build-fa1mzstq\n",
            "  Resolved https://github.com/hukkelas/DSFD-Pytorch-Inference.git to commit dde9c7dd9cdc9254c2ca345222c86a8ecfa17f5b\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_detection==0.2.1) (1.26.4)\n",
            "Building wheels for collected packages: face_detection\n",
            "  Building wheel for face_detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face_detection: filename=face_detection-0.2.1-py3-none-any.whl size=29972 sha256=2d93c2e24fc20b50d0c467e15a1ca74262da23a0d96ec942e002ef1a67957eae\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-l2r0ix1l/wheels/31/fc/c5/28af01da09c7625bd966f9871b081cb72e131ffb926c0de66b\n",
            "Successfully built face_detection\n",
            "Installing collected packages: face_detection\n",
            "Successfully installed face_detection-0.2.1\n",
            "Requirement already satisfied: face_detection in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_detection) (1.26.4)\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.20-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.8.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.25.5)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Downloading mediapipe-0.10.20-cp310-cp310-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.10.20 sounddevice-0.5.1\n",
            "Requirement already satisfied: dlib in /usr/local/lib/python3.10/dist-packages (19.24.2)\n",
            "pacotes instalados\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importando os pacotes utilizados"
      ],
      "metadata": {
        "id": "dHymk3FauK4t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "eDA1up8gsJma"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import bz2\n",
        "import os\n",
        "import face_detection\n",
        "import mediapipe as mp\n",
        "from scipy.spatial import distance as dist\n",
        "import dlib\n",
        "from PIL import Image\n",
        "import warnings\n",
        "from google.colab import output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#Exibição na mesma tela do Jupyter\n",
        "%matplotlib inline\n",
        "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
        "\n",
        "# pacotes utilizados para tirar foto/videos\n",
        "from IPython.display import display, Javascript, Image, HTML\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import threading\n",
        "\n",
        "dlib.DLIB_USE_CUDA = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# capturando foto base para reconhecimento facial\n",
        "def capturar_foto_colab():\n",
        "    \"\"\"Captura uma imagem no Google Colab usando a webcam.\"\"\"\n",
        "    display(Javascript('''\n",
        "        async function captureImage() {\n",
        "            const div = document.createElement('div');\n",
        "            const captureButton = document.createElement('button');\n",
        "            captureButton.textContent = 'Capture';\n",
        "            div.appendChild(captureButton);\n",
        "            const video = document.createElement('video');\n",
        "            video.style.display = 'block';\n",
        "            const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "            document.body.appendChild(div);\n",
        "            div.appendChild(video);\n",
        "            video.srcObject = stream;\n",
        "            await video.play();\n",
        "            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "            await new Promise((resolve) => captureButton.onclick = resolve);\n",
        "            const canvas = document.createElement('canvas');\n",
        "            canvas.width = video.videoWidth;\n",
        "            canvas.height = video.videoHeight;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            stream.getVideoTracks()[0].stop();\n",
        "            div.remove();\n",
        "            return canvas.toDataURL('image/jpeg').split(',')[1];\n",
        "        }\n",
        "    '''))\n",
        "\n",
        "    with open('foto_base.jpg', 'wb') as f:\n",
        "        f.write(b64decode(output.eval_js('captureImage()')))\n",
        "    print(\"Imagem capturada e salva como 'foto_base.jpg'.\")\n",
        "\n",
        "capturar_foto_colab()"
      ],
      "metadata": {
        "id": "-gA4kVYjQnjA",
        "outputId": "7ba5b588-50c6-4ce1-c4e4-21f2fe0a2a62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        async function captureImage() {\n",
              "            const div = document.createElement('div');\n",
              "            const captureButton = document.createElement('button');\n",
              "            captureButton.textContent = 'Capture';\n",
              "            div.appendChild(captureButton);\n",
              "            const video = document.createElement('video');\n",
              "            video.style.display = 'block';\n",
              "            const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "            document.body.appendChild(div);\n",
              "            div.appendChild(video);\n",
              "            video.srcObject = stream;\n",
              "            await video.play();\n",
              "            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "            await new Promise((resolve) => captureButton.onclick = resolve);\n",
              "            const canvas = document.createElement('canvas');\n",
              "            canvas.width = video.videoWidth;\n",
              "            canvas.height = video.videoHeight;\n",
              "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "            stream.getVideoTracks()[0].stop();\n",
              "            div.remove();\n",
              "            return canvas.toDataURL('image/jpeg').split(',')[1];\n",
              "        }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagem capturada e salva como 'foto_base.jpg'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importando os modelos utilizados no reconhecimento facial"
      ],
      "metadata": {
        "id": "S4g0MfQAFh7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# criando diretorios para armazenar as informacoes necessarias\n",
        "os.makedirs('videos', exist_ok=True) # validacao de video\n",
        "os.makedirs('foto', exist_ok=True) # validacao de video"
      ],
      "metadata": {
        "id": "gUpHEK6QGCZO"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# URL e nome dos arquivos\n",
        "landmark_url = \"http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\"\n",
        "landmark_file_name = \"shape_predictor_68_face_landmarks.dat.bz2\"\n",
        "landmark_dat_file = \"shape_predictor_68_face_landmarks.dat\"\n",
        "\n",
        "recognition_url = \"http://dlib.net/files/dlib_face_recognition_resnet_model_v1.dat.bz2\"\n",
        "recognition_file_name = \"dlib_face_recognition_resnet_model_v1.dat.bz2\"\n",
        "recognition_dat_file = \"dlib_face_recognition_resnet_model_v1.dat\"\n",
        "\n",
        "# Criando diretório para salvar os modelos\n",
        "os.makedirs('dlib_models', exist_ok=True)\n",
        "\n",
        "# Apontando para o diretório\n",
        "os.chdir('dlib_models')\n",
        "\n",
        "# Função para baixar e extrair arquivos\n",
        "def download_and_extract(url, compressed_file_name, extracted_file_name):\n",
        "    # Baixando o arquivo se já não estiver baixado\n",
        "    if not os.path.exists(compressed_file_name):\n",
        "        os.system(f\"wget {url}\")\n",
        "\n",
        "    # Extraindo o arquivo .dat se já não estiver extraído\n",
        "    if not os.path.exists(extracted_file_name):\n",
        "        with bz2.BZ2File(compressed_file_name, \"rb\") as f_in, open(extracted_file_name, \"wb\") as f_out:\n",
        "            f_out.write(f_in.read())\n",
        "\n",
        "        # Apagando o arquivo compactado\n",
        "        os.remove(compressed_file_name)\n",
        "\n",
        "# Baixando e extraindo o arquivo de landmarks\n",
        "download_and_extract(landmark_url, landmark_file_name, landmark_dat_file)\n",
        "\n",
        "# Baixando e extraindo o arquivo de reconhecimento facial\n",
        "download_and_extract(recognition_url, recognition_file_name, recognition_dat_file)\n",
        "\n",
        "# Voltando para o diretório original\n",
        "os.chdir('..')\n",
        "\n",
        "print('Modelos baixados e extraídos com sucesso!')"
      ],
      "metadata": {
        "id": "ptCQRHaKFEui",
        "outputId": "b5315a75-8d0c-46de-a057-91f91f4e244e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelos baixados e extraídos com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tirando uma fotografia e extraindo caracteristicas para aplicar o face recognition"
      ],
      "metadata": {
        "id": "3i-2QrZQFw6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# caminho das pastas\n",
        "save_folder = \"/content/foto\"\n",
        "os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "# funcao para capturar foto usando javascript\n",
        "def take_photo(quality=0.8):\n",
        "    js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture Photo';\n",
        "      capture.style.marginBottom = '10px';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      div.appendChild(video);\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Wait for the user to click the capture button\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getTracks().forEach(track => track.stop());\n",
        "      div.remove();\n",
        "\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "    display(js)\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    return binary\n",
        "\n",
        "# capturar a foto\n",
        "photo_binary = take_photo()\n",
        "\n",
        "# nome do arquivo\n",
        "file_name = input(\"Enter the file name (with .jpg extension): \")\n",
        "\n",
        "# garantindo que esta salvo como jpg\n",
        "if not file_name.lower().endswith('.jpg'):\n",
        "    file_name += '.jpg'\n",
        "\n",
        "file_path = os.path.join(save_folder, file_name)\n",
        "with open(file_path, 'wb') as f:\n",
        "    f.write(photo_binary)\n",
        "\n",
        "print(f\"Photo saved to {file_path}\")"
      ],
      "metadata": {
        "id": "c3ct-hZIGRe5",
        "outputId": "99dbee8e-3104-493e-e7eb-f8cb2b6c9de8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function takePhoto(quality) {\n",
              "      const div = document.createElement('div');\n",
              "      const capture = document.createElement('button');\n",
              "      capture.textContent = 'Capture Photo';\n",
              "      capture.style.marginBottom = '10px';\n",
              "      div.appendChild(capture);\n",
              "\n",
              "      const video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      div.appendChild(video);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      // Wait for the user to click the capture button\n",
              "      await new Promise((resolve) => capture.onclick = resolve);\n",
              "\n",
              "      const canvas = document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "      stream.getTracks().forEach(track => track.stop());\n",
              "      div.remove();\n",
              "\n",
              "      return canvas.toDataURL('image/jpeg', quality);\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the file name (with .jpg extension): lucas_poggetto\n",
            "Photo saved to /content/foto/lucas_poggetto.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oLXMKwm_PRVJ",
        "outputId": "bce17ba6-7380-4522-d233-4d09c10b7653",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/foto/lucas_poggetto.jpg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validar_reconhecimento_colab(foto_base_path=\"file_path\"):\n",
        "    \"\"\"Valida o reconhecimento facial comparando a imagem capturada no Colab com a imagem base.\"\"\"\n",
        "    try:\n",
        "        detector = dlib.get_frontal_face_detector()\n",
        "        reconhecedor = dlib.face_recognition_model_v1(\"dlib_face_recognition_resnet_model_v1.dat\")\n",
        "        predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "    except RuntimeError as e:\n",
        "        print(\"Erro ao carregar os modelos do dlib. Certifique-se de que a versão CPU está instalada corretamente ou desative o uso de CUDA.\")\n",
        "        print(e)\n",
        "        return\n",
        "\n",
        "    # Carregar imagem base e extrair descritor\n",
        "    imagem_base = cv2.imread(foto_base_path)  # Corrigir indentação aqui\n",
        "    faces_base = detectar_face(imagem_base, detector)\n",
        "\n",
        "    if len(faces_base) == 0:\n",
        "        raise Exception(\"Nenhuma face detectada na imagem base.\")\n",
        "\n",
        "    shape_base = predictor(imagem_base, faces_base[0])\n",
        "    descritor_base = reconhecedor.compute_face_descriptor(imagem_base, shape_base)\n",
        "\n",
        "    print(\"Capturando imagem para validação.\")\n",
        "    capturar_foto_colab()\n",
        "\n",
        "    imagem_live = cv2.imread('foto_base.jpg')\n",
        "    faces_live = detectar_face(imagem_live, detector)\n",
        "\n",
        "    if len(faces_live) == 0:\n",
        "        raise Exception(\"Nenhuma face detectada na imagem ao vivo.\")\n",
        "\n",
        "    shape_live = predictor(imagem_live, faces_live[0])\n",
        "    descritor_live = reconhecedor.compute_face_descriptor(imagem_live, shape_live)\n",
        "\n",
        "    # Comparar os descritores\n",
        "    distancia = np.linalg.norm(np.array(descritor_base) - np.array(descritor_live))\n",
        "    print(f\"Distância entre os descritores: {distancia:.2f}\")\n",
        "\n",
        "    if distancia < 0.6:\n",
        "        print(\"Faces correspondem! Acesso permitido.\")\n",
        "    else:\n",
        "        print(\"Faces não correspondem! Acesso negado.\")"
      ],
      "metadata": {
        "id": "OKawepzZPNzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JavaScript code to capture video\n",
        "html_code = \"\"\"\n",
        "<video id=\"video\" width=\"640\" height=\"480\" autoplay></video>\n",
        "<button id=\"startButton\">Start Recording</button>\n",
        "<button id=\"stopButton\" disabled>Stop Recording</button>\n",
        "<video id=\"playback\" width=\"640\" height=\"480\" controls></video>\n",
        "<a id=\"downloadLink\" download=\"recorded-video.webm\"></a>\n",
        "\n",
        "<script>\n",
        "    const video = document.querySelector('#video');\n",
        "    const playback = document.querySelector('#playback');\n",
        "    const downloadLink = document.querySelector('#downloadLink');\n",
        "    const startButton = document.querySelector('#startButton');\n",
        "    const stopButton = document.querySelector('#stopButton');\n",
        "\n",
        "    navigator.mediaDevices.getUserMedia({ video: true }).then((stream) => {\n",
        "        video.srcObject = stream;\n",
        "        let mediaRecorder;\n",
        "        const chunks = [];\n",
        "\n",
        "        startButton.onclick = () => {\n",
        "            mediaRecorder = new MediaRecorder(stream);\n",
        "            mediaRecorder.start();\n",
        "            startButton.disabled = true;\n",
        "            stopButton.disabled = false;\n",
        "\n",
        "            mediaRecorder.ondataavailable = (event) => chunks.push(event.data);\n",
        "            mediaRecorder.onstop = () => {\n",
        "                const blob = new Blob(chunks, { type: 'video/webm' });\n",
        "                playback.src = URL.createObjectURL(blob);\n",
        "                downloadLink.href = playback.src;\n",
        "                downloadLink.textContent = 'Download Video';\n",
        "                startButton.disabled = false;\n",
        "                stopButton.disabled = true;\n",
        "            };\n",
        "        };\n",
        "\n",
        "        stopButton.onclick = () => mediaRecorder.stop();\n",
        "    });\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "# Display the HTML interface\n",
        "display(HTML(html_code))"
      ],
      "metadata": {
        "id": "UR6wrfItjxqb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Montando detector de faces\n",
        "\n",
        "Utilizando o algoritmo DSFDDetector para detectar uma face"
      ],
      "metadata": {
        "id": "ANvOYVy9WL0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculo_distancia_olhos(eye):\n",
        "    # Calcula a distância euclidiana entre os pontos dos olhos\n",
        "    A = dist.euclidean(eye[1], eye[5])\n",
        "    B = dist.euclidean(eye[2], eye[4])\n",
        "    C = dist.euclidean(eye[0], eye[3])\n",
        "    # Calcula a razão de aspecto do olho\n",
        "    ear = (A + B) / (2.0 * C)\n",
        "    return ear\n",
        "\n",
        "def detector_faces_video(video_path, output_path, threshhold_ear=0.3):\n",
        "    # Criando o detector de faces\n",
        "    detector = face_detection.build_detector(\n",
        "        \"DSFDDetector\",\n",
        "        confidence_threshold=0.5,\n",
        "        nms_iou_threshold=0.3\n",
        "    )\n",
        "    # utilizando algoritmo de 68 pontos faciais\n",
        "    preditor_marcos_faciais = dlib.shape_predictor(\n",
        "        '/content/dlib_models/shape_predictor_68_face_landmarks.dat'\n",
        "        )\n",
        "\n",
        "    # abrindo o arquivo de video\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # propriedades do video\n",
        "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'VP80')  # WebM codec\n",
        "    out = cv2.VideoWriter(output_path, fourcc, 15, (width, height))\n",
        "\n",
        "    while True:\n",
        "        ret, frame = video.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # convertendo para RGB\n",
        "        rgb_frame = frame[:, :, ::-1]\n",
        "\n",
        "        # detectando  face\n",
        "        detections = detector.detect(rgb_frame)\n",
        "\n",
        "        # desenhando o retangulo na face\n",
        "        for det in detections:\n",
        "            x1, y1, x2, y2, score = det\n",
        "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
        "\n",
        "            # Draw a red rectangle by default\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "\n",
        "            # utilizando dlib para detectar a face\n",
        "            dlib_rect = dlib.rectangle(x1,y1,x2,y2)\n",
        "\n",
        "            # marcos faciais\n",
        "            landmarks = preditor_marcos_faciais(rgb_frame, dlib_rect)\n",
        "\n",
        "            # coordenadas dos olhos\n",
        "            left_eye = [(landmarks.part(i).x, landmarks.part(i).y) for i in range(36, 42)]\n",
        "            right_eye = [(landmarks.part(i).x, landmarks.part(i).y) for i in range(42, 48)]\n",
        "\n",
        "            # calculando a distancia para ambos os olhos\n",
        "            olho_esq = calculo_distancia_olhos(left_eye)\n",
        "            olho_dir = calculo_distancia_olhos(right_eye)\n",
        "            ear = (olho_esq + olho_dir) / 2\n",
        "\n",
        "            # validacao de olhos piscando\n",
        "            blink_detected = ear < threshhold_ear\n",
        "\n",
        "            # Check if blink is detected\n",
        "            if ear < threshhold_ear:\n",
        "                # Update the rectangle to green if liveness is proven\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                cv2.putText(frame, 'Liveness OK', (x1, y1 - 10),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "        # retorno do video\n",
        "        out.write(frame)\n",
        "\n",
        "    # Release resources\n",
        "    video.release()\n",
        "    out.release()\n",
        "    print(f\"Input video FPS: {fps}\")\n",
        "    print(f\"Processed video saved to: {output_path}\")\n",
        "\n",
        "# Example usage\n",
        "input_video_path = '/content/videos/recorded-video.webm'\n",
        "output_video_path = '/content/videos/recorded-video_processado.webm'\n",
        "\n",
        "# chamada da funcao criada anteriormente\n",
        "detector_faces_video(input_video_path, output_video_path, threshhold_ear = 0.3)"
      ],
      "metadata": {
        "id": "tCX9NZJRZS6k",
        "outputId": "de960a31-c97a-4b6b-d9c3-2ebfbdfa81b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input video FPS: 62.5\n",
            "Processed video saved to: /content/videos/recorded-video_processado.webm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sj3_H-hSuvxk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}